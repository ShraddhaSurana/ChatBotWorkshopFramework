{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import useful libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re, pprint\n",
    "# import sys\n",
    "import os\n",
    "import csv\n",
    "# import argparse\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import random\n",
    "from nltk.classify import SklearnClassifier\n",
    "# from sklearn.naive_bayes import BernoulliNB\n",
    "# from sklearn.svm import SVC\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence = \"The big brown fox jumped over a lazy dog.\"\n",
    "sentence2 = \"Buffalo buffalo Buffalo buffalo buffalo buffalo Buffalo buffalo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abcdefgh\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'the big brown fox jumped over a lazy dog.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert sentence to lower case\n",
    "'This' == 'this'\n",
    "print('AbcdEFgH'.lower())\n",
    "sentence.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize - extract individual words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'big', 'brown', 'fox', 'jumped', 'over', 'a', 'lazy', 'dog']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter words to remove non-useful words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'big', 'brown', 'fox', 'jumped', 'lazy', 'dog']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_words = [w for w in tokens if not w in stopwords.words('english')]\n",
    "filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    filtered_words = [w for w in tokens if not w in stopwords.words('english')]\n",
    "    return filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['big', 'brown', 'fox', 'jumped', 'lazy', 'dog']\n"
     ]
    }
   ],
   "source": [
    "preprocessed_sentence = preprocess(sentence)\n",
    "print(preprocessed_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('big', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumped', 'VBD'), ('lazy', 'JJ'), ('dog', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "tags = nltk.pos_tag(preprocessed_sentence)\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting only Nouns and Verb nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_features(sentences):\n",
    "    features = []\n",
    "    for tagged_word in sentences:\n",
    "        word, tag = tagged_word\n",
    "        if tag=='NN' or tag == 'VBN' or tag == 'NNS' or tag == 'VBP' or tag == 'RB' or tag == 'VBZ' or tag == 'VBG' or tag =='PRP' or tag == 'JJ':\n",
    "            features.append(word)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['big', 'brown', 'fox', 'lazy', 'dog']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_features(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatize words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cactus\n",
      "willing\n",
      "foot\n",
      "stemmed\n"
     ]
    }
   ],
   "source": [
    "lmtzr = WordNetLemmatizer()\n",
    "print lmtzr.lemmatize('cacti')\n",
    "print lmtzr.lemmatize('willing')\n",
    "print lmtzr.lemmatize('feet')\n",
    "print lmtzr.lemmatize('stemmed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stem words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_for_stemming = ['stem', 'stemming', 'stemmed', 'stemmer', 'stems','feet','willing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'stem', u'stem', u'stem', u'stemmer', u'stem', u'feet', u'will']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "[stemmer.stem(x) for x in words_for_stemming]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_feature(text):\n",
    "    words = preprocess(text)\n",
    "#     print('words: ',words)\n",
    "    tags = nltk.pos_tag(words)\n",
    "#     print('tags: ',tags)\n",
    "    extracted_features = extract_features(tags)#rename to extract keys based on tags\n",
    "#     print('Extracted features: ',extracted_features)\n",
    "    stemmed_words = [stemmer.stem(x) for x in extracted_features]\n",
    "#     print(stemmed_words)\n",
    "    # join with space\n",
    "#     return (\" \".join(stemmed_words))\n",
    "   \n",
    "    return stemmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'big', u'brown', u'fox', u'lazi', u'dog']\n"
     ]
    }
   ],
   "source": [
    "words = extract_feature(sentence)\n",
    "print words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_feats(words):\n",
    "    return dict([(word, True) for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'big': True, u'brown': True, u'dog': True, u'fox': True, u'lazi': True}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_feats(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing the whole document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_feature_from_doc(data):\n",
    "    result = []\n",
    "    corpus = []\n",
    "    for (text,category,answer) in data:\n",
    "#         corpus.append(text)\n",
    "#         print(corpus)\n",
    "        features = extract_feature(text)\n",
    "#         if len(features1) != 0:\n",
    "#             document_words = set(features1)\n",
    "#             features = {}\n",
    "#             for word in document_words:\n",
    "#                 features['contains(%s)' % word] = (word in document_words)\n",
    "#             return features\n",
    "#         print('\\n', features)\n",
    "        corpus.append(features)\n",
    "        result.append((word_feats(features), category))\n",
    "#         result.append((\" \".join(features), answer))\n",
    "    return (result, sum(corpus,[]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\\n', [u'text'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([({u'text': True}, 'category')], [u'text'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_feature_from_doc([['this is text','category','answer to give']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_content(filename):\n",
    "    doc = os.path.join(filename)\n",
    "    with open(doc, 'r') as content_file:\n",
    "        lines = csv.reader(content_file,delimiter='|')\n",
    "        data = [x for x in lines if len(x) == 3]\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = 'leaves.txt'\n",
    "data = get_content(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features_data, corpus = extract_feature_from_doc(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({u'good': True, u'morn': True}, 'Morning')\n"
     ]
    }
   ],
   "source": [
    "print(features_data[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a model using these fetures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## split data into train and test sets\n",
    "split_ratio = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_dataset(data, split_ratio):\n",
    "    random.shuffle(data)\n",
    "    data_length = len(data)\n",
    "    train_split = int(data_length * split_ratio)\n",
    "    return (data[:train_split]), (data[train_split:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_data, test_data = split_dataset(features_data, split_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_using_decision_tree(training_data, test_data):\n",
    "    # entropy_cutoff=0.1,support_cutoff=0.7 gives awesome results\n",
    "    classifier = nltk.classify.DecisionTreeClassifier.train(training_data, verbose = True)\n",
    "#     print(classifier)\n",
    "#     print(classifier.pretty_format(width=70, prefix='', depth=4))\n",
    "    classifier_name = type(classifier).__name__\n",
    "    training_set_accuracy = nltk.classify.accuracy(classifier, training_data)\n",
    "    print('training set accuracy: ', training_set_accuracy)\n",
    "    test_set_accuracy = nltk.classify.accuracy(classifier, test_data)\n",
    "    print('test set accuracy: ', test_set_accuracy)\n",
    "    return classifier, classifier_name, test_set_accuracy, training_set_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best stump for    116 toks uses option               err=0.6983\n",
      "best stump for     76 toks uses forward              err=0.6579\n",
      "best stump for     62 toks uses taken                err=0.6613\n",
      "best stump for     51 toks uses annual               err=0.6078\n",
      "best stump for     36 toks uses leav                 err=0.6111\n",
      "best stump for     24 toks uses help                 err=0.6250\n",
      "best stump for     18 toks uses name                 err=0.5000\n",
      "best stump for     15 toks uses great                err=0.4667\n",
      "best stump for     13 toks uses thank                err=0.3846\n",
      "best stump for     11 toks uses even                 err=0.3636\n",
      "best stump for     10 toks uses good                 err=0.3000\n",
      "best stump for      6 toks uses thank                err=0.3333\n",
      "best stump for     12 toks uses type                 err=0.0000\n",
      "best stump for     15 toks uses None                 err=0.2000\n",
      "best stump for     11 toks uses annual               err=0.0000\n",
      "best stump for     40 toks uses taken                err=0.2250\n",
      "best stump for     28 toks uses use                  err=0.2143\n",
      "best stump for     25 toks uses None                 err=0.2400\n",
      "('training set accuracy: ', 0.8793103448275862)\n",
      "('test set accuracy: ', 0.9)\n"
     ]
    }
   ],
   "source": [
    "classifier, classifier_name, test_set_accuracy, training_set_accuracy = train_using_decision_tree(training_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the data\n",
    "np.save('training_data', training_data)\n",
    "np.save('test_data', test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# training_data = np.load('training_data.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "option=None? .......................................... Default-Balance-Annual-Leaves\n",
      "  forward=None? ....................................... Default-Balance-Annual-Leaves\n",
      "    taken=None? ....................................... Default-Balance-Annual-Leaves\n",
      "      annual=None? .................................... Default-Balance-Annual-Leaves\n",
      "      annual=True? .................................... Balance-Annual-Leaves\n",
      "    taken=True? ....................................... Utilized-Annual-Leaves\n",
      "      annual=None? .................................... Default-Utilized-Annual-Leaves\n",
      "      annual=True? .................................... Utilized-Annual-Leaves\n",
      "  forward=True? ....................................... CF\n",
      "option=True? .......................................... Utilized-Optional-Leaves\n",
      "  taken=None? ......................................... Utilized-Optional-Leaves\n",
      "    use=None? ......................................... Balance-Optional-Leaves\n",
      "    use=True? ......................................... Utilized-Optional-Leaves\n",
      "  taken=True? ......................................... Utilized-Optional-Leaves\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classifier.pretty_format())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
