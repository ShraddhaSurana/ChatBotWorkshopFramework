{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a chat bot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Will have to do :\n",
    "install python;\n",
    "install jupyter notebook\n",
    "do pip install on alot of things!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Import stuff\n",
    "import nltk, re, pprint\n",
    "import sys\n",
    "import os\n",
    "import csv\n",
    "import argparse\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import random\n",
    "from nltk.classify import SklearnClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.svm import SVC\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file = 'leaves.txt'\n",
    "\n",
    "# leaves_data = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Stem words\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## split data into train and test sets\n",
    "split_ratio = 0.8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get contents from file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_content(filename):\n",
    "    test_doc = os.path.join(filename)\n",
    "    with open(test_doc, 'r') as content_file:\n",
    "        lines = csv.reader(content_file,delimiter='|')\n",
    "        res = [x for x in lines if len(x) == 3]\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = get_content(file_to_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Greetings'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This data is a list\n",
    "data[1]\n",
    "data[1][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence = \"The big brown fox jumped over a lazy dog.\"\n",
    "sentence2 = \"Buffalo buffalo Buffalo buffalo buffalo buffalo Buffalo buffalo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abcdefgh\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'the big brown fox jumped over a lazy dog.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert sentence to lower case\n",
    "'This' == 'this'\n",
    "print('AbcdEFgH'.lower())\n",
    "sentence.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'big', 'brown', 'fox', 'jumped', 'over', 'a', 'lazy', 'dog']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract individual words\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'big', 'brown', 'fox', 'jumped', 'lazy', 'dog']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_words = [w for w in tokens if not w in stopwords.words('english')]\n",
    "filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "  def preprocess(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    filtered_words = [w for w in tokens if not w in stopwords.words('english')]\n",
    "    return filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['big', 'brown', 'fox', 'jumped', 'lazy', 'dog']\n"
     ]
    }
   ],
   "source": [
    "preprocessed_sentence = preprocess(sentence)\n",
    "print(preprocessed_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('big', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumped', 'VBD'), ('lazy', 'JJ'), ('dog', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "tags = nltk.pos_tag(preprocessed_sentence)\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting only Nouns and Verb nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_features(sentences):\n",
    "    features = []\n",
    "    for tagged_word in sentences:\n",
    "        word, tag = tagged_word\n",
    "        if tag=='NN' or tag == 'VBN' or tag == 'NNS' or tag == 'VBP' or tag == 'RB' or tag == 'VBZ' or tag == 'VBG' or tag =='PRP' or tag == 'JJ':\n",
    "            features.append(word)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['big', 'brown', 'fox', 'lazy', 'dog']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_features(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Lemmatize words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cactus'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lmtzr = WordNetLemmatizer()\n",
    "lmtzr.lemmatize('cacti')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'willing'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lmtzr.lemmatize('willing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'foot'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lmtzr.lemmatize('feet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stemmed'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lmtzr.lemmatize('stemmed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stem words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_for_stemming = ['stem', 'stemming', 'stemmed', 'stemmer', 'stems','feet','willing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stem', 'stem', 'stem', 'stemmer', 'stem', 'feet', 'will']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[stemmer.stem(x) for x in words_for_stemming]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#using porter stemmer\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stem', 'stem', 'stem', 'stemmer', 'stem', 'feet', 'will']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "[stemmer.stem(x) for x in words_for_stemming]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def get_feature_set(sent_keys):\n",
    "#     return {'keywords': ' '.join(sent_keys)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "Extracting features from one sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_feature(text):\n",
    "    words = preprocess(text)\n",
    "#     print('words: ',words)\n",
    "    tags = nltk.pos_tag(words)\n",
    "#     print('tags: ',tags)\n",
    "    extracted_features = extract_features(tags)#rename to extract keys based on tags\n",
    "#     print('Extracted features: ',extracted_features)\n",
    "    stemmed_words = [stemmer.stem(x) for x in extracted_features]\n",
    "#     print(stemmed_words)\n",
    "    return stemmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['big', 'brown', 'fox', 'lazi', 'dog']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_feature(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Parsing the whole document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_feature_from_doc(data):\n",
    "    result = []\n",
    "    corpus = []\n",
    "    for (text,category,answer) in data:\n",
    "#         corpus.append(text)\n",
    "#         print(corpus)\n",
    "        features = extract_feature(text)\n",
    "        corpus.append(features)\n",
    "        # features.append((sent_features, category))\n",
    "        result.append((features, answer))\n",
    "    return (result, sum(corpus,[]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_content(filename):\n",
    "    doc = os.path.join(filename)\n",
    "    with open(doc, 'r') as content_file:\n",
    "        lines = csv.reader(content_file,delimiter='|')\n",
    "        data = [x for x in lines if len(x) == 3]\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = get_content(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d, corpus = extract_feature_from_doc(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'hi', 'hello', 'hi', 'hi', 'hi', 'hey', 'hello', 'hi', 'hey', 'hey', 'hi', 'hey', 'hello', 'good', 'morn', 'good', 'afternoon', 'good', 'even', 'good', 'night', '_', '__', 'today', 'want', 'help', 'need', 'help', 'help', 'want', 'help', 'want', 'assist', 'help', 'name', 'tell', 'name', 'name', 'great', 'talk', 'great', 'thank', 'help', 'thank', 'thank', 'much', 'thank', 'thank', 'much', 'mani', 'type', 'leav', 'type', 'leav', 'type', 'leav', 'type', 'leav', 'type', 'mani', 'leav', 'taken', 'mani', 'leav', 'alreadi', 'taken', 'mani', 'annual', 'leav', 'mani', 'annual', 'leav', 'taken', 'mani', 'annual', 'leav', 'alreadi', 'taken', 'annual', 'leav', 'count', 'taken', 'mani', 'annual', 'leav', 'taken', 'number', 'annual', 'leav', 'taken', 'annual', 'leav', 'taken', 'number', 'annual', 'leav', 'alreadi', 'taken', 'annual', 'leav', 'taken', 'annual', 'leav', 'alreadi', 'taken', 'number', 'annual', 'leav', 'taken', 'number', 'annual', 'leav', 'taken', 'annual', 'leav', 'annual', 'leav', 'mani', 'option', 'leav', 'taken', 'mani', 'option', 'leav', 'option', 'leav', 'taken', 'number', 'option', 'leav', 'taken', 'option', 'leav', 'count', 'taken', 'number', 'option', 'leav', 'taken', 'number', 'option', 'leav', 'taken', 'number', 'option', 'leav', 'taken', 'mani', 'option', 'leav', 'taken', 'number', 'option', 'leav', 'taken', 'number', 'option', 'leav', 'alreadi', 'taken', 'option', 'leav', 'taken', 'option', 'leav', 'alreadi', 'taken', 'number', 'option', 'leav', 'number', 'option', 'leav', 'option', 'leav', 'count', 'use', 'option', 'leav', 'count', 'use', 'option', 'leav', 'count', 'use', 'option', 'leav', 'count', 'use', 'option', 'leav', 'count', 'taken', 'option', 'leav', 'count', 'taken', 'option', 'leav', 'count', 'taken', 'mani', 'option', 'leav', 'mani', 'option', 'leav', 'number', 'option', 'leav', 'option', 'leav', 'mani', 'leav', 'mani', 'leav', 'take', 'mani', 'leav', 'remain', 'remain', 'leav', 'leav', 'tell', 'leav', 'balanc', 'remain', 'leav', 'leav', 'balanc', 'leav', 'pend', 'annual', 'leav', 'count', 'remain', 'mani', 'annual', 'leav', 'annual', 'leav', 'annual', 'leav', 'balanc', 'annual', 'leav', 'mani', 'annual', 'leav', 'remain', 'annual', 'leav', 'remain', 'remain', 'annual', 'leav', 'annual', 'leav', 'remain', 'number', 'annual', 'leav', 'remain', 'number', 'annual', 'leav', 'annual', 'leav', 'remain', 'annual', 'leav', 'count', 'remain', 'annual', 'leav', 'count', 'remain', 'annual', 'leav', 'count', 'remain', 'annual', 'leav', 'count', 'remain', 'annual', 'leav', 'balanc', 'annual', 'leav', 'balanc', 'annual', 'leav', 'number', 'annual', 'leav', 'remain', 'number', 'annual', 'leav', 'remain', 'mani', 'option', 'leav', 'remain', 'option', 'leav', 'remain', 'number', 'option', 'leav', 'remain', 'mani', 'option', 'leav', 'mani', 'option', 'leav', 'option', 'leav', 'balanc', 'option', 'leav', 'mani', 'option', 'leav', 'take', 'option', 'leav', 'take', 'mani', 'option', 'leav', 'option', 'leav', 'balanc', 'option', 'leav', 'count', 'remain', 'option', 'leav', 'option', 'leav', 'option', 'leav', 'balanc', 'option', 'option', 'leav', 'count', 'remain', 'option', 'leav', 'count', 'remain', 'option', 'leav', 'count', 'remain', 'number', 'option', 'leav', 'remain', 'number', 'option', 'leav', 'remain', 'number', 'option', 'leav', 'remain', 'number', 'option', 'leav', 'number', 'option', 'leav', 'mani', 'carri', 'forward', 'leav', 'number', 'carri', 'forward', 'leav', 'tell', 'carri', 'forward', 'leav', 'count', 'number', 'forward', 'leav', 'number', 'forward', 'leav', 'number', 'forward', 'leav', 'mani', 'carri', 'forward', 'previou', 'year', 'tell', 'carri', 'forward', 'leav', 'previou', 'year', 'tell', 'carri', 'forward', 'leav', 'tell', 'carri', 'forward', 'leav', 'forward', 'leav', 'carri', 'forward', 'leav', 'carri', 'forward', 'previou', 'year', 'carri', 'forward', 'leav']\n"
     ]
    }
   ],
   "source": [
    "print(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-155-cac45eb44495>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0md\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(min_df=1)\n",
    "vectorizer                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Following is an example of the corpus\n",
    "# corpus = [\n",
    "# ...     'This is the first document.',\n",
    "# ...     'This is the second second document.',\n",
    "# ...     'And the third one.',\n",
    "# ...     'Is this the first document?',\n",
    "# ... ]\n",
    "# corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<432x38 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 431 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = vectorizer.fit_transform(corpus)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze = vectorizer.build_analyzer()\n",
    "analyze(\"This is a text document to analyze.\") == (['this', 'is', 'text', 'document', 'to', 'analyze'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__',\n",
       " 'afternoon',\n",
       " 'alreadi',\n",
       " 'annual',\n",
       " 'assist',\n",
       " 'balanc',\n",
       " 'carri',\n",
       " 'count',\n",
       " 'even',\n",
       " 'forward',\n",
       " 'good',\n",
       " 'great',\n",
       " 'hello',\n",
       " 'help',\n",
       " 'hey',\n",
       " 'hi',\n",
       " 'leav',\n",
       " 'mani',\n",
       " 'morn',\n",
       " 'much',\n",
       " 'name',\n",
       " 'need',\n",
       " 'night',\n",
       " 'number',\n",
       " 'option',\n",
       " 'pend',\n",
       " 'previou',\n",
       " 'remain',\n",
       " 'take',\n",
       " 'taken',\n",
       " 'talk',\n",
       " 'tell',\n",
       " 'thank',\n",
       " 'today',\n",
       " 'type',\n",
       " 'use',\n",
       " 'want',\n",
       " 'year']"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_.get('leav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !! words that were not seen in the training corpus will be completely ignored in future calls to the transform method\n",
    "\n",
    "# To do next - try Bi-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
